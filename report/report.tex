\documentclass{article}
\usepackage{verbatim}
\usepackage{fancyvrb}
\usepackage{colortbl}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{changepage}
\usepackage{enumitem}
\usepackage{gensymb}
\usepackage{csquotes}
\usepackage{float}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage[hidelinks]{hyperref}
\usepackage[
	left=1in,
	right=1in,
	top=1in,
	bottom=1in
]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage[
	backend=bibtex,
	style=authoryear,
	citestyle=authoryear
]{biblatex}
\addbibresource{~/.config/assets/LaTeX/auni.bib}

\setboolean{@twoside}{false}
\setlength{\parskip}{1em}
\setlength{\parindent}{4em}
\renewcommand{\baselinestretch}{1}
%\makeatletter
%\renewcommand{\@seccntformat}[1]{}
%\makeatother

\theoremstyle{definition}
\newtheorem{exmp}{Example}[section]

\author{Chris Sobczak}
\title{Survey of Operating Systems Used by World Universities}

\begin{document}
\maketitle
\begin{flushleft}



\section{Introduction}
The goal of the survey is to determine the market share
of that linux and other open source operating systems
occupy in the academic setting. So the
figure of interest is the proportion of machines that
universities run for thier websites, mail servers and other
digital services that run an open source operating system.

\section{Methodologies}
I have used a dataset of all known domains registered by
universities around the world (\cite{Hipo}). I have taken a simple random
sample of these schools and then extracted all of the corresponding
subdomains.

For example, Simon Fraser University (SFU) has \url{sfu.ca} registered with
the Canadian Internet Registration Authority (CIRA) and
they have thousands of subdomains under \url{sfu.ca} such as \url{mail.sfu.ca},
\url{canvas.sfu.ca} and \url{go.sfu.ca}. A lot of the subdomains do not
contain relevant web content or are otherwise unused, so to filter out these
subdomains before trying to identify the operating systems being
used to run the surver at that address, I have used the \texttt{host} DNS
lookup tool and other methods for checking if there are actual services running
at that address. More about the tools I used in \autoref{sec:tools} \hyperref[sec:tools]{Tools}.

\subsection{Sampling Frame}


\subsection{Sample Size Selection}
$e=0.03$ and $\alpha=0.05$ \cite{lohr2019}

We want to estimate the proportion of university servers
run an open source operating system using a 95\% confidence
interval with a margin of error of 0.03.

\cite{lohr2019} page 47:
``In surveys in which one of the main responses of interest
is a proportion, it is often easiest to use that response
in setting the sample size.
For large populations, $S^2 \approx p(1-p)$, which
attains its maximal value when $p=1/2$. So using
$n_0=1.96^2/(4e^2)$ will result in a 95\% CI with width at most
$2e$.''

$$
	n_0
	=
	\frac{
		z^2_{\alpha/2}S^2
	}{
		e^2
	}
	=
	\frac{
		1.96^2(\frac{1}{2})(1-\frac{1}{2})
	}{
		e^2
	}
	\approx
	1067
$$

No need to use the fpc adjustment since the sample size
is reasonable compared to the population size.

\section{Tools} \label{sec:tools}
For taking the raw json file and selecting the sample, I used R and the \texttt{rjson}
package. All project source files can be found at this
\href{https://github.com/chrissobczak/os-survey}{GitHub} repository.
The R script outputs the base second and third level subdomains
into a file with one domain per line, for which I run \texttt{findomain -t}.
This tool takes a domain and searches various databases and tests the domain
for subdomains associated with it. I take this and output all the subdomains
into a file for each school, and then test if the service is up.

Documentation for the tools can be found in the Appendix

\section{Sample Selection}
Some of the schools in my sampling frame had more than one associated
domain, so when I expanded all the sampled domains into one file, one
domain per line, I ended up with 1087 domains to attempt to extract
subdomains from.

When extracting the subdomains, my script only processed 1079 domains.
Three of these domains that were not processed were \texttt{aloma.edu},
\texttt{student.uts.edu.au}, and \texttt{www.clcmn.edu}. In the case of
\texttt{aloma.edu}, this is just a typo in my sampling frame for the
Alamo Colleges domain, which naturally is corrected to \texttt{alamo.edu}.
The next missing domain \texttt{student.uts.edu.au}, for the University
of Technology Sydney in Australia which is just a subdomain for their
website \texttt{uts.edu.au}. Finally, \texttt{www.clcmn.edu} for
Central Lakes College-Brainerd is another subdomain for their college
that was already extracted from \texttt{clcmn.edu}.

In the list of domains, there were 16 duplicated domains and this
also accounts for the difference in number of domains that went
through the \texttt{gen-domains} script and the original sampled
list. Here is a list of the duplicated domains: \texttt{aku.edu},
\texttt{bashedu.ru}, \texttt{most.gov.mm}, and \texttt{uwo.ca}.

After removing the duplicated domains and generating the domains
for the corrected \texttt{alamo.edu}, I ended up with a full set
of 1081 lists of subdomains.

In the github repository, the file \texttt{undup-domains} is the audited file containing all
of the highest level domains for the sample.

The next step will be ensuring that two related domains did not go through
the process, for example making sure that we did not run \texttt{gen-domains}
on \texttt{sfu.ca} and \texttt{mail.sfu.ca}, since this would likely result in
duplicated information.

Using the line \texttt{cat subdomain/* | uniq -d} we can see that
there were no repeated lines.

\section{Extracting Info}
\texttt{curl -I}


\end{flushleft}
\printbibliography
\end{document}
