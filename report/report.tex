\documentclass{article}
\usepackage{verbatim}
\usepackage{fancyvrb}
\usepackage{colortbl}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{changepage}
\usepackage{enumitem}
\usepackage{gensymb}
\usepackage{csquotes}
\usepackage{float}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage[hidelinks]{hyperref}
\usepackage[
	left=1in,
	right=1in,
	top=1in,
	bottom=1in
]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage[
	backend=bibtex,
	style=authoryear,
	citestyle=authoryear
]{biblatex}
\addbibresource{~/.config/assets/LaTeX/auni.bib}

\setboolean{@twoside}{false}
\setlength{\parskip}{1em}
\setlength{\parindent}{4em}
\renewcommand{\baselinestretch}{1}
%\makeatletter
%\renewcommand{\@seccntformat}[1]{}
%\makeatother

\theoremstyle{definition}
\newtheorem{exmp}{Example}[section]

\author{Chris Sobczak}
\title{Survey of University Server Software}

\begin{document}
\begin{titlepage}
\maketitle

\vspace*{\fill}

\section{Abstract}
Universities, especially in the United States have consistently
increased the cost of attendance for their students with
no transparent cause. Some investigation and speculation
includes administrative costs, tenure, research expenses
and increasing security threats (\cite{ronald2009}).
A nominally large amount of money is spent on proprietary
software licenses for services like Microsoft Office 360,
SAS, SPSS and licenses for their own web services like
Microsoft Express, Red Hat Enterprise Linux, and other
server infrastructure.
The goal of the survey is to determine the market share
that open source server software occupies in the academic
setting.
The
figure to be estimated is the proportion of university web
services licenses. Are universities taking advantage of
the secure, free, and open source industry standard technologies
like nginx and Apache?
This study target population includes
all computers that
universities run for their websites, mail servers and other
digital services that provide a portal for students and
members of the public to access. The main focus of the study
is on those public domain services, and not services like
ftp servers and other internal affairs.

\end{titlepage}


\begin{flushleft}
\begin{multicols}{2}

\section{Background}
Open source software or ``free and open source software'' (FOSS)
is a piece or package of software
that is distributed with the source code available for
auditting and editting. The statistical software R is a
great example of a flourishing FOSS project where
users are able to contribute to the project and create new
packages. Another example is the OpenBSD operating system.
In the case of OpenBSD and other FOSS operating systems,
the public has the ability to know exactly what is going
on behind the scenes. This is not to say that every user
is required to read and understand the low level source
code, but it is a principle that the project is transparent
and the software community can audit it, creating a web of trust.

In contrast, there is no reason for a consumer to trust
in the word of a company that distributes closed source
software. This organization can claim whatever they want
about the security and privacy of their software but
without having the right to audit the source code,
there is no reason to believe any of those claims.
This raises security concerns in proprietary software since
if the only way a software package is considered \textit{secure}
is by the fact that no one knows how it works except the producer,
then as soon as the code becomes available (due to a leak), it can
all be considered compromised. This happens constantly with the Microsoft
Windows operating system, as we have seen most recently with the Solar
Winds attack (\cite{solarwinds2021}).
FOSS is secure through industry standard
security schemes, cryptography and by the knowledge of the thousands
of programmers who have access to it (compared to the relatively
smaller number of people who work on a single companies development
team).

Finally, maybe the more important reason for institutions,
FOSS is also (most of the time) free of cost (in addition
to ``free as in freedom'').
Universities and other public institutions
spend millions of dollars each year on software licenses
for their students, staff and web services and other
infrastructure. Increasing costs for post-secondary education and
healthcare administration costs can be attributed to some extent to
these rising software licensing prices and the constant
need for cybersecurity improvements. The increasing costs
make higher education less accessible and harm the most
vulnerable populations who rely on affordable healthcare.
A very obvious way to reduce
costs is be replacing these proprietary packages with equivalent
and more secure, free, and open source software (\cite{kenny2000}).

In addition to the problamatic cost of closed source, proprietary
software, these costly, licensed software packages are most vulnerable
to security threats.
Security should be a high priority of all organizations but
especially at public institutions where all of the software that staff and
students interact with poses a threat to those individuals.
Providing one for profit company exclusive access to your staff and
students, in the age of survailliance capitalism is quite reckless
(\cite{karen2018}).

So this study, establishes a base-line estimate
of the proportion of free and open source servers being used by
universities around the world.
The questions that are answered include:
Are there specific countries whose universities
use more open source servers than others? What kinds of services
are associated with certain operating systems?

\section{Methods}
This survey was conducted using cluster sampling.
The primary sampling unit (psu) being the name
of an institution, and the primary unit of interest
being the associated domain(s). Each of the schools
highest level registered domain names are probed to
extract all their associated subdomains, defining the secondary
sampling units (ssus).
A census was then taken of these ssus
to determine the software run at each public domain name and
calculate the proportion of the school's services that
run open source software.

To provide an example, if Simon Fraser University (SFU)
was draw into our sample, the unit of interest would be
their registered domain name \texttt{sfu.ca}. Using the
tool \texttt{findomain}, all of the subdomains under
\texttt{sfu.ca} are collected into a file and probed to see if the service is up.
Some example subdomains are \texttt{mailgate.sfu.ca},
\texttt{imapnew.sfu.ca}, and \texttt{canvas.sfu.ca} for
the schools email services and canvas portal. This is
only a very short list of some of the possible subdomains.
It is common to have thousands of subdomains associated
to the same top name for each school. Finally, filtering out all of
the services that are not up, the header of the service is pulled
using \texttt{curl}, which includes information about the software
running at that address. The results are cleaned aggregated and proportions
of the resulting categories are tabulated.
Details on the tools I used in \autoref{sec:tools} \hyperref[sec:tools]{Tools}.

For example, pulling the server information from \texttt{mail.sfu.ca} for
SFU's mail service results in \texttt{Microsoft-IIS/10.0} as it is a
Microsoft Exchange mail server. All responses including this, \texttt{MS},
\texttt{Win64}, and other server names that are licensed with anything
except the \textit{GPL}, \textit{BSD}, \textit{MIT}, or \textit{Apache}
licenses are considered proprietary, whereas those responses with
the mentioned licenses are considered open source.

\subsection{Survey Design}

Using cluster
sampling, an SRS is taken of all the schools in the sampling
frame (\cite{Hipo}). This sampling frame is an open source dataset
of most of the universities in the world. Inevitably
this list will have omitted some schools, but the set
contains 9693 schools which is slightly on the low
end of estimates, which may introduce bias in the
conclusions of this study.

\subsection{Sample Size Selection}
The goal of this study is
to estimate the proportion of university servers
running an open source operating system using a 95\% confidence
interval with a margin of error of 0.03.
Therefore, using
\cite{lohr2019},
``\dots surveys in which one of the main responses of interest
is a proportion, it is often easiest to use that response
in setting the sample size.
For large populations, $S^2 \approx p(1-p)$, which
attains its maximal value when $p=1/2$. So using
$n_0=1.96^2/(4e^2)$ will result in a 95\% CI with width at most
$2e$.''

$$
	n_0
	=
	\frac{
		z^2_{\alpha/2}S^2
	}{
		e^2
	}
	=
	\frac{
		1.96^2(\frac{1}{2})(1-\frac{1}{2})
	}{
		e^2
	}
	\approx
	1067
$$

No need to use the finite population correction adjustment
since the sample size
is reasonable compared to the population size and the full
dataset can be collected with a reasonable amount of resources.

\subsection{Taking the Sample}
With an appropriate sample size of $n=1067$,
using an R script to
draw the sample, the selected school domains
are saved in the \texttt{data.Rda} file found
in the GitHub repository, along with \texttt{sample.R},
used to draw the sample.
Within the sample, 1049 schools have only
one registered domain, 16 schools have two
registered domains and 2 have three
domains.

$$
	N=9693, \ n=1067, \ m_0\approx2539008
$$

The following 18 schools that were drawn in the sample
have more than one domain name registered:
Augusta University,
University of Manchester,
Universidad del Pa√≠s Vasco,
Chinju National University of Education,
Royal Holloway and Bedford New College,
Northeastern University,
University of the Pacific,
Kwangju University,
Kwangwoon University,
University of Massachusetts at Lowell,
St. Mary's University,
University of Essex,
Chonnam National University,
Hanshin University,
Savannah College of Art and Design,
University of Technology Sydney,
Universitat Pompeu Fabra, and
Kyungil University.

These 16 schools with two domains and 2 schools with three domains
accounts for the total 1087 ($1067 + (16 \textrm{ duplicates}) +
(2\times2 \textrm{ more duplicates}) = 1087$) domains in the
\texttt{domains} file of the GitHub repository. This file
contained a few duplicates and subdomains for the school,
so some of the domains were repeated and some were just included within
the schools \texttt{subdomains/} file, reducing it to 1081 files
in \texttt{subdomains/}.

All domains were separated onto their own line of the \texttt{probing/domains}
file and processed with \texttt{findomain}.
When extracting the subdomains, the \texttt{gen-subdomains}
script only processed 1079 domains, identifying an inconsistency.
Three of these domains that were not processed were \texttt{aloma.edu},
\texttt{student.uts.edu.au}, and \texttt{www.clcmn.edu}. In the case of
\texttt{aloma.edu}, this is just a typo in the sampling frame for the
Alamo Colleges' domain, which naturally is corrected to \texttt{alamo.edu}.
The next missing domain \texttt{student.uts.edu.au}, for the University
of Technology Sydney in Australia which is just a subdomain for their
website \texttt{uts.edu.au}, identified as a duplicate domain.
Finally, \texttt{www.clcmn.edu} for
Central Lakes College-Brainerd is another subdomain for their college
that was already extracted from \texttt{clcmn.edu}, another duplicated domain.

In the github repository, the file \texttt{domains} is the audited file containing all
of the highest level domains for the sample that the full observational information could
be extracted. Domains omitted from this file were unreachable as mentioned in the discussion.
From this file, the domains belonging to the same school have been concatinated
so that results can be organized by psu before calculating proportions.


\section{Results}
The full results dataset of proportion of servers at
each psu is available on the github as \texttt{proportions.csv}.


\textbf{table about the findings, make some graphs to put in the appendix \dots}


\subsection{Discussion and Conclusion}
\textbf{Any anticpiated shortcomings in your study desgin
and their impact on your conclusions about the questions
of interest.}

\subsection{Bias}
Although the sample size for the study is very large, some bias in the sample
could contribute to errors in the conclusions. One possible source of bias
is the sampling frame potentially not including \textit{all} universities in
the world. Inevitably, this is the case, however it seems reasonable to consider
the sample draw from the frame is representative of the target population
(244 schools from the US, 23 from Canada, 10 from Italy, and so on).

Another potential source of bias in this study includes a few technical
limitations. First, the \texttt{findomain} tool relies on databases and
other historical DNS records and methods to identify subdomains, but it
could miss some, most likely missing newly registered subdomains which
may not represent the true makeup of the schools services. Additional
bias could be introduced with \texttt{curl}, when it is not able
to make a connection, or the header of the document not included the required
server information for this study, the response is counted as an \texttt{NA}.
However, it is almost always the case, that these
missed, and unidentified servers are internal services, observational units that
are outside the scope of the target population, not considered in this study.

Finally, a few of the schools in the sample were unreachable for probing,
particularly schools far from the location that the study was conducted
and schools in countries under strict internet policies like China and
Saudi Arabia. The large sample size helps limit the impact of these
non-responses, however further attempts could be taken to pull that
information, given more time and resources.



\section{Tools} \label{sec:tools}
\textbf{\underline{go one by one with subsections explaining the tools used
for the project, curl, findomain, and maybe talk about nmapper; maybe move to the appendix?}}
For taking the raw json file and selecting the sample, I used R and the \texttt{rjson}
package. All project source files can be found at this
\href{https://github.com/chrissobczak/os-survey}{GitHub} repository.
The R script outputs the base second and third level subdomains
into a file with one domain per line, for which I run \texttt{findomain -t}.
This tool takes a domain and searches various databases and tests the domain
for subdomains associated with it. I take this and output all the subdomains
into a file for each school, and then test if the service is up.

Documentation for the tools can be found in the Appendix
\subsection{Extracting Info From SSUs}
\texttt{curl -I}

\end{multicols}
\end{flushleft}
\printbibliography
\section{Appendix}
\end{document}
